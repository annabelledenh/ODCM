{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe291b5c",
   "metadata": {
    "id": "fe291b5c"
   },
   "source": [
    "# Mediamarkt scraper\n",
    "\n",
    "This Mediamarkt scraper can be used to scrape data from four categories of the website Mediamarkt.com: \n",
    "- Mobile phones\n",
    "- Televisions\n",
    "- Laptops\n",
    "- tablets\n",
    "\n",
    "Two files (csv and json) will be exported once scraped which includes data of the brands in the categories such as the prices, availabiliy, ratings, reviews and other attributes.\n",
    "\n",
    "If all steps are followed, this will take up around 45 minutes of your time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffe8ff",
   "metadata": {
    "id": "f0ffe8ff"
   },
   "source": [
    "### Import packages\n",
    "\n",
    "First, import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fa32b",
   "metadata": {
    "id": "cc0fa32b"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa946a",
   "metadata": {
    "id": "6afa946a"
   },
   "source": [
    "### Collect page URLS\n",
    "\n",
    "Next, we will collect all page URLs of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2efadc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2efadc2",
    "outputId": "33405d8c-a56b-4868-9d8a-49b454e94c1c"
   },
   "outputs": [],
   "source": [
    "# Define function to check whether there is a next page\n",
    "def check_next_page(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    next_btn = soup.find(class_= \"pagination-next\") \n",
    "    return next_btn.find(\"a\").attrs[\"href\"] if next_btn else None\n",
    "print(\"Function created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf5578",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21bf5578",
    "outputId": "d614b970-2f22-4932-e53e-9993000abcdf"
   },
   "outputs": [],
   "source": [
    "# Define a function to collect all page urls\n",
    "def generate_page_urls(page_url):\n",
    "    page_urls = []\n",
    "    while page_url:\n",
    "        print(\"Saving: \", page_url)\n",
    "        page_urls.append(page_url)\n",
    "        if check_next_page(page_url) != None: \n",
    "            page_url = \"https://www.mediamarkt.nl\" + check_next_page(page_url)\n",
    "        else:\n",
    "            break\n",
    "    print(\"Done with this category!\")\n",
    "    \n",
    "    sleep(1)\n",
    "    \n",
    "    return page_urls\n",
    "print(\"Function created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf567b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cf567b3",
    "outputId": "da176738-8de0-4a2e-d7be-6a6e8f3d1010",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the generate_page_urls function to collect all the page urls of the categories you want to scrape\n",
    "\n",
    "# 1. Define the first page of every category\n",
    "smartphones_url = \"https://www.mediamarkt.nl/nl/category/_smartphones-483222.html?page=1\"\n",
    "laptops_url = \"https://www.mediamarkt.nl/nl/category/_laptops-482723.html?page=1\"\n",
    "tablets_url = \"https://www.mediamarkt.nl/nl/category/_tablets-645048.html?page=1\"\n",
    "tvs_url = \"https://www.mediamarkt.nl/nl/category/_televisies-450682.html?page=1\"\n",
    "\n",
    "# 2. Use the function on all categories, first checking whether there is a next page and if so, adding it to page_urls\n",
    "page_urls = generate_page_urls(smartphones_url) + generate_page_urls(laptops_url) + generate_page_urls(tablets_url) + generate_page_urls(tvs_url)\n",
    "print(\"All page URLS have been collected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84131e",
   "metadata": {
    "id": "cb84131e"
   },
   "source": [
    "### Collect product page URLS\n",
    "\n",
    "All page URLs are collected, so now the product page URLs can be collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9556fd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9556fd5",
    "outputId": "3293741c-800b-424f-8b5f-6f5787d0db24"
   },
   "outputs": [],
   "source": [
    "# Create a function to collect the product_urls\n",
    "def create_product_urls(page_urls):\n",
    "    product_urls = []\n",
    "    for page_url in page_urls:\n",
    "        res = requests.get(page_url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        products = soup.find_all(\"h2\")\n",
    "        \n",
    "        for product in products:\n",
    "            try:\n",
    "                product_url = \"https://www.mediamarkt.nl\" + product.find(\"a\").attrs[\"href\"]\n",
    "                product_urls.append(product_url)\n",
    "                print(\"Saving \" + product_url)\n",
    "            except:\n",
    "                print(\"this is no product\")\n",
    "            \n",
    "        sleep(1)\n",
    "        \n",
    "    return product_urls\n",
    "print(\"Function created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4a04b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ee4a04b",
    "outputId": "b698b160-80cf-47b4-9636-c6fb70beeb45"
   },
   "outputs": [],
   "source": [
    "# Use the create_product_urls function to create a list product_urls\n",
    "product_urls = create_product_urls(page_urls)\n",
    "print(\"All product URLS have been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d1cb2",
   "metadata": {
    "id": "156d1cb2"
   },
   "source": [
    "### Collect product specific data\n",
    "\n",
    "From these product page URLs, we will now collect all product specific data. This includes the before mentioned data: prices, availabiliy, ratings, reviews and other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bceb79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27bceb79",
    "outputId": "80c54bac-0847-45f7-d000-ab07a2c67747",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search for the right elements, store them in variables an put them together in a dictionary\n",
    "product_data = []\n",
    "\n",
    "def scrape(product_urls):\n",
    "    for url in product_urls:\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\") \n",
    "        device_type = soup.find(class_ = \"breadcrumbs\").find_all(\"li\")[2].text.replace(\"\\n\", \"\")\n",
    "        names = soup.find(class_ = \"stickable\").img.attrs[\"alt\"]\n",
    "        try:\n",
    "            prices = soup.find(\"div\", class_ = \"price big\").text\n",
    "        except:\n",
    "            prices = \"no price\"\n",
    "        try:\n",
    "            instock = soup.find(class_ = \"box infobox availability\").meta.attrs[\"content\"]\n",
    "        except: \n",
    "            instock = \"OutOfStock\"\n",
    "        try:\n",
    "            ratings = soup.find(class_ = \"bvseo-ratingValue\").text\n",
    "        except:\n",
    "            ratings = \"no rating\"\n",
    "        try: \n",
    "            reviews = soup.find(class_ = \"bvseo-reviewCount\").text\n",
    "        except: \n",
    "            reviews = \"no reviews\"\n",
    "    \n",
    "        # Get product attributes and store them in attributes_json\n",
    "        attributes = soup.find(class_ = \"specification\").find_all('dt')\n",
    "        values = soup.find(class_ = \"specification\").find_all('dd')\n",
    "\n",
    "        attributes_json = {}\n",
    "        for x, y in zip(attributes, values):\n",
    "            attributes_json[x.text]=y.text\n",
    "    \n",
    "        # Store all variables in products\n",
    "        products = {\"device_type\": device_type, \"name\": names, \"price\": prices, \"instock\": instock, \"rating\": ratings, \n",
    "                    \"nr_reviews\": reviews, \"attributes\": attributes_json}\n",
    "        product_data.append(products)\n",
    "        print(\"Saving: \", products[\"name\"])\n",
    "\n",
    "    sleep(1)\n",
    "    \n",
    "    return(product_data)\n",
    "print(\"Function created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469224e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "469224e9",
    "outputId": "dffc93ee-4612-4fb8-ea29-f59b1c923095",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the scraping function to save the data in product_data\n",
    "product_data = scrape(product_urls)\n",
    "print(\"All product data have been collected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f476f11",
   "metadata": {
    "id": "0f476f11"
   },
   "source": [
    "### Store and export the product data\n",
    "\n",
    "Now all data is collected, the raw data, JSON file and csv file will be created for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb92c74",
   "metadata": {
    "id": "4fb92c74"
   },
   "outputs": [],
   "source": [
    "# Write the raw data, product_data, to a json_file\n",
    "with open('raw_product_data.json', 'w') as json_file:\n",
    "  json.dump(product_data, json_file)\n",
    "print(\"Data have been saved in a json file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa5ba1",
   "metadata": {
    "id": "59fa5ba1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Opening and normalizing the raw JSON data\n",
    "\n",
    "# 1. Open the saved raw JSON data and convert it into a pandas dataframe\n",
    "df = pd.read_json('raw_product_data.json')\n",
    "\n",
    "# 2. Normalize the data, putting the before nested items in attributes into columns and dropping the column 'attributes'\n",
    "df = df.join(pd.json_normalize(df.attributes)).drop(columns=['attributes'])\n",
    "print(\"Dataframe has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d6d5c",
   "metadata": {
    "id": "1c8d6d5c"
   },
   "outputs": [],
   "source": [
    "# Write the pandas dataframe to a csv\n",
    "df.to_csv(\"mediamarkt_scraper_output.csv\", sep = \",\", index = False)\n",
    "print(\"Done, the csv is ready for data preparation.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "scraper_Mediamarkt_team7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
